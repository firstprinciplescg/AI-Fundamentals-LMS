# Prompting systems (TCREI) and evaluation loops

**Audience:** Founders, owners, and small-business executives  
**Prerequisites:** None

## What you’ll learn
- Author prompts that are reliable and re-usable
- Close the loop with rubrics and error analysis
- Adopt a library that the whole team can use

## Key topics we cover
- Task, Context, References, Evaluate, Iterate (TCREI)
- Prompt patterns for research, writing, and analysis
- Rubrics and golden answers for evaluation
- Maintaining prompt libraries and variants

## Executive overview
This lesson translates complex AI concepts into practical management decisions. We focus on how to evaluate opportunities, reduce risk, and move from pilot experiments to measurable, repeatable workflows. You’ll see what matters strategically, what can be delegated, and what to insist on when teams propose AI-enabled solutions.


## How it works (plain-English)
We keep the mechanics simple: inputs (your task, data, rules) go into a model or agent, the system reasons with tools and constraints, and then returns an output. When the task requires action—sending emails, updating a CRM, generating documents—an **automation layer** or **agent runtime** executes steps under guardrails (permissions, reviews, logs).

## Common pitfalls to avoid
- One-off prompts that nobody can reproduce
- No explicit evaluation; drift over time
- Ignoring edge cases and failure modes

## What to measure
- Quality score vs. rubric, hallucination rate
- Coverage of reference docs and citations


## Next steps
Run the associated hands‑on exercise and capture baseline metrics this week. Bring at least one “keep/kill/iterate” decision to the next session.
